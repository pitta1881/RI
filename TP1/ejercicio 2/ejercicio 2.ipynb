{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_DIR = '../RI-tknz-data'\n",
    "STOP_WORDS_FILE = '../stop-words.txt'\n",
    "MAX_LONG = 20\n",
    "MIN_LONG = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFileStopWords(stop_words_file: str):\n",
    "    complete_route = os.path.abspath(stop_words_file)\n",
    "    with open(complete_route, 'r', encoding='utf-8') as file:\n",
    "        stop_words = set(word.strip() for word in file.readlines())\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(words_list, stop_words_list):\n",
    "    cleaned_text = [word for word in words_list if word not in stop_words_list]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageTermLength(terms):\n",
    "    if not terms:  # Verificar si la lista está vacía\n",
    "        return 0\n",
    "    total_length = sum(len(term) for term in terms)  # Sumar las longitudes de todos los términos\n",
    "    return total_length / len(terms)  # Calcular el promedio dividiendo la suma por la cantidad de términos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortestAndLargestDoc(docs):\n",
    "    min_tokens = float('inf')\n",
    "    max_tokens = 0\n",
    "    llave_min_tokens = None\n",
    "    llave_max_tokens = None\n",
    "\n",
    "    for llave, tupla in docs.items():\n",
    "        cantidad_tokens, _ = tupla\n",
    "        if cantidad_tokens < min_tokens:\n",
    "            min_tokens = cantidad_tokens\n",
    "            llave_min_tokens = llave\n",
    "        if cantidad_tokens > max_tokens:\n",
    "            max_tokens = cantidad_tokens\n",
    "            llave_max_tokens = llave\n",
    "\n",
    "    return docs[llave_min_tokens], docs[llave_max_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_terminos(diccionario):\n",
    "    # Ordenar en orden ascendente para los términos menos frecuentes\n",
    "    dic_less_freq = sorted(diccionario.items(), key=lambda x: x[1])\n",
    "    # Ordenar en orden descendente para los términos más frecuentes\n",
    "    dic_more_freq = sorted(diccionario.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Seleccionar los 10 primeros términos\n",
    "    return dic_less_freq[:10], dic_more_freq[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTermsToFile(output_file: str, word_freq_total, word_freq_docs):\n",
    "    word_freq_total_ordered = dict(sorted(word_freq_total.items()))\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for term in word_freq_total_ordered:\n",
    "            file.write(f\"{term} {word_freq_total[term]} {word_freq_docs[term]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeStadisticsToFile(output_file: str, total_docs, total_tokens, total_terms, key_list_terms, shortestAndLargestDoc, one_time_terms):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"{total_docs}\\n\")\n",
    "        file.write(f\"{total_tokens} {total_terms}\\n\")\n",
    "        file.write(f\"{total_tokens * 100 / total_tokens} {total_terms * 100 / total_tokens}\\n\")\n",
    "        file.write(f\"{averageTermLength(key_list_terms)}\\n\")\n",
    "        file.write(f\"{shortestAndLargestDoc[0][0]} {shortestAndLargestDoc[0][1]} {shortestAndLargestDoc[1][0]} {shortestAndLargestDoc[1][1]}\\n\")\n",
    "        file.write(f\"{one_time_terms}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeFrequencyToFile(output_file: str, word_freq_docs, firstTenTerms):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for term, _ in firstTenTerms[0]:\n",
    "            file.write(f\"{term} {word_freq_docs[term]}\\n\")\n",
    "        for term, _ in firstTenTerms[1]:\n",
    "            file.write(f\"{term} {word_freq_docs[term]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAbbreviations(content):\n",
    "    REGEX_ABBREVIATIONS = r'\\b[a-zA-Z][bcdfgh-np-tvxz]+(?![A-Z])\\.'\n",
    "    return re.findall(REGEX_ABBREVIATIONS, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findEmailsAndUrls(content):\n",
    "    REGEX_EMAILS = r'\\b[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
    "    REGEX_URLS = r'(https?://)?(www\\.[a-z0-9]+(?:\\.[a-z0-9]+)+)'\n",
    "    list_emails = re.findall(REGEX_EMAILS, content)\n",
    "    list_urls = [''.join(url_tuple) for url_tuple in re.findall(REGEX_URLS, content)]\n",
    "    return list_emails + list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNumbersAndPhones(content):\n",
    "    REGEX_NUMBERS = r' (\\d+)'\n",
    "    REGEX_PHONES = r'\\+?\\d{6,}|\\+?\\d{8,}|\\d{2,}-\\d{5,}|\\+?\\d+-\\d{2,}-\\d{5,}'\n",
    "    list_numbers = re.findall(REGEX_NUMBERS, content)\n",
    "    list_phones = re.findall(REGEX_PHONES, content)\n",
    "    return list_phones + list_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWords(content):\n",
    "    REGEX_WORDS = f'[A-zÀ-ú]{{{MIN_LONG},{MAX_LONG}}}'\n",
    "    return re.findall(REGEX_WORDS, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(content, stop_words_list = None):\n",
    "    abbreviations_list = findAbbreviations(content)\n",
    "    emailsAndUrls_list = findEmailsAndUrls(content)\n",
    "    numbersAndPhones_list = findNumbersAndPhones(content)\n",
    "    words_list = findWords(content)\n",
    "    content = abbreviations_list + emailsAndUrls_list + numbersAndPhones_list + words_list\n",
    "    if stop_words_list != None:\n",
    "        content = removeStopWords(content, stop_words_list)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir(directory: str, stopWordsFile = ''):\n",
    "    stop_words_list = loadFileStopWords(stopWordsFile) if stopWordsFile != '' else None\n",
    "    directory = os.path.abspath(directory)\n",
    "    total_tokens = 0\n",
    "    total_docs = len(os.listdir(directory))\n",
    "    word_freq_total = {}\n",
    "    word_freq_docs = {}\n",
    "    docs = {}\n",
    "    for file in os.listdir(directory):\n",
    "        complete_route = os.path.join(directory, file)\n",
    "        with open(complete_route, 'r', encoding='utf8') as f:\n",
    "            doc_count_terms = 0\n",
    "            content = f.read()\n",
    "            content = tokenizer(content, stop_words_list)\n",
    "            total_tokens += len(content)\n",
    "            unique_words_in_file = set(content)\n",
    "            for word in unique_words_in_file:\n",
    "                word_freq_total[word] = word_freq_total.get(word, 0) + content.count(word)\n",
    "                word_freq_docs[word] = word_freq_docs.get(word, 0) + 1\n",
    "                doc_count_terms += content.count(word)\n",
    "            docs[file] = (len(content), doc_count_terms)\n",
    "    return word_freq_total, word_freq_docs, total_docs, total_tokens, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_freq_total, word_freq_docs = tokenizer(COLLECTION_DIR)\n",
    "word_freq_total, word_freq_docs, total_docs, total_tokens, docs = process_dir(COLLECTION_DIR, STOP_WORDS_FILE)\n",
    "writeTermsToFile('terminos.txt', word_freq_total, word_freq_docs)\n",
    "writeStadisticsToFile('estadisticas.txt', total_docs, total_tokens, len(word_freq_total), list(word_freq_total.keys()), shortestAndLargestDoc(docs), len([value for value in word_freq_total.values() if value == 1]))\n",
    "writeFrequencyToFile('frequencia.txt', word_freq_docs, obtener_terminos(word_freq_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
