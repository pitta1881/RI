{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     C:\\Users\\pitta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package snowball_data is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('snowball_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_DIR = '../RI-tknz-data'\n",
    "STOP_WORDS_FILE = '../stop-words.txt'\n",
    "MAX_LONG = 20\n",
    "MIN_LONG = 3\n",
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFileStopWords(stop_words_file: str):\n",
    "    complete_route = os.path.abspath(stop_words_file)\n",
    "    with open(complete_route, 'r', encoding='utf-8') as file:\n",
    "        stop_words = set(word.strip() for word in file.readlines())\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(words_list: List[str], stop_words_list: List[str]):\n",
    "    cleaned_text = [word for word in words_list if word not in stop_words_list]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageTermLength(terms):\n",
    "    if not terms:  # Verificar si la lista está vacía\n",
    "        return 0\n",
    "    total_length = sum(len(term) for term in terms)  # Sumar las longitudes de todos los términos\n",
    "    return total_length / len(terms)  # Calcular el promedio dividiendo la suma por la cantidad de términos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortestAndLargestDoc(docs):\n",
    "    min_tokens = float('inf')\n",
    "    max_tokens = 0\n",
    "    llave_min_tokens = None\n",
    "    llave_max_tokens = None\n",
    "\n",
    "    for llave, tupla in docs.items():\n",
    "        cantidad_tokens, _ = tupla\n",
    "        if cantidad_tokens < min_tokens:\n",
    "            min_tokens = cantidad_tokens\n",
    "            llave_min_tokens = llave\n",
    "        if cantidad_tokens > max_tokens:\n",
    "            max_tokens = cantidad_tokens\n",
    "            llave_max_tokens = llave\n",
    "\n",
    "    return docs[llave_min_tokens], docs[llave_max_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_terminos(diccionario):\n",
    "    # Ordenar en orden ascendente para los términos menos frecuentes\n",
    "    dic_less_freq = sorted(diccionario.items(), key=lambda x: x[1])\n",
    "    # Ordenar en orden descendente para los términos más frecuentes\n",
    "    dic_more_freq = sorted(diccionario.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Seleccionar los 10 primeros términos\n",
    "    return dic_less_freq[:10], dic_more_freq[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTermsToFile(output_file: str, word_freq_total, word_freq_docs):\n",
    "    word_freq_total_ordered = dict(sorted(word_freq_total.items()))\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for term in word_freq_total_ordered:\n",
    "            file.write(f\"{term} {word_freq_total[term]} {word_freq_docs[term]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeStadisticsToFile(output_file: str, total_docs, total_tokens, total_terms, key_list_terms, shortestAndLargestDoc, one_time_terms):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"{total_docs}\\n\")\n",
    "        file.write(f\"{total_tokens} {total_terms}\\n\")\n",
    "        file.write(f\"{total_tokens * 100 / total_tokens} {total_terms * 100 / total_tokens}\\n\")\n",
    "        file.write(f\"{averageTermLength(key_list_terms)}\\n\")\n",
    "        file.write(f\"{shortestAndLargestDoc[0][0]} {shortestAndLargestDoc[0][1]} {shortestAndLargestDoc[1][0]} {shortestAndLargestDoc[1][1]}\\n\")\n",
    "        file.write(f\"{one_time_terms}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeFrequencyToFile(output_file: str, word_freq_docs, firstTenTerms):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for term, _ in firstTenTerms[0]:\n",
    "            file.write(f\"{term} {word_freq_docs[term]}\\n\")\n",
    "        for term, _ in firstTenTerms[1]:\n",
    "            file.write(f\"{term} {word_freq_docs[term]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWords(content):\n",
    "    REGEX_WORDS = f'[A-zÀ-ú0-9]{{{MIN_LONG},{MAX_LONG}}}'\n",
    "    return re.findall(REGEX_WORDS, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useStemmer(content):\n",
    "    return [stemmer.stem(word) for word in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(content, stop_words_list):\n",
    "    content = findWords(content)\n",
    "    content = useStemmer(content)\n",
    "    if stop_words_list != None:\n",
    "        content = removeStopWords(content, stop_words_list)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir(directory: str, stopWordsFile = ''):\n",
    "    stop_words_list = loadFileStopWords(stopWordsFile) if stopWordsFile != '' else None\n",
    "    directory = os.path.abspath(directory)\n",
    "    total_tokens = 0\n",
    "    total_docs = len(os.listdir(directory))\n",
    "    word_freq_total = {}\n",
    "    word_freq_docs = {}\n",
    "    docs = {}\n",
    "    for file in os.listdir(directory):\n",
    "        complete_route = os.path.join(directory, file)\n",
    "        with open(complete_route, 'r', encoding='utf8') as f:\n",
    "            doc_count_terms = 0\n",
    "            content = f.read()\n",
    "            total_tokens += len(content)\n",
    "            unique_words_in_file = set(tokenizer(content, stop_words_list))\n",
    "            for word in unique_words_in_file:\n",
    "                if len(word) >= MIN_LONG and len(word) <= MAX_LONG:\n",
    "                    word_freq_total[word] = word_freq_total.get(word, 0) + content.count(word)\n",
    "                    word_freq_docs[word] = word_freq_docs.get(word, 0) + 1\n",
    "                    doc_count_terms += content.count(word)\n",
    "            docs[file] = (len(content), doc_count_terms)\n",
    "    return word_freq_total, word_freq_docs, total_docs, total_tokens, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_freq_total, word_freq_docs = tokenizer(COLLECTION_DIR)\n",
    "word_freq_total, word_freq_docs, total_docs, total_tokens, docs = process_dir(COLLECTION_DIR, STOP_WORDS_FILE)\n",
    "writeTermsToFile('terminos.txt', word_freq_total, word_freq_docs)\n",
    "writeStadisticsToFile('estadisticas.txt', total_docs, total_tokens, len(word_freq_total), list(word_freq_total.keys()), shortestAndLargestDoc(docs), len([value for value in word_freq_total.values() if value == 1]))\n",
    "writeFrequencyToFile('frequencia.txt', word_freq_docs, obtener_terminos(word_freq_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
