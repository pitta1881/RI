{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../../shared\"))  # Agrega la carpeta al PYTHONPATH\n",
    "from tokenizer_v2 import tokenizer, loadFileStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_DIR = '../RI-tknz-data'\n",
    "# COLLECTION_DIR = './TestCollection_ER2'\n",
    "STOP_WORDS_FILE = '../../shared/stop-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageTermLength(terms):\n",
    "    if not terms:  # Verificar si la lista está vacía\n",
    "        return 0\n",
    "    total_length = sum(len(term) for term in terms)  # Sumar las longitudes de todos los términos\n",
    "    return total_length / len(terms)  # Calcular el promedio dividiendo la suma por la cantidad de términos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortestAndLargestDoc(docs):\n",
    "    min_tokens = float('inf')\n",
    "    max_tokens = 0\n",
    "    llave_min_tokens = None\n",
    "    llave_max_tokens = None\n",
    "\n",
    "    for llave, tupla in docs.items():\n",
    "        cantidad_tokens, _ = tupla\n",
    "        if cantidad_tokens < min_tokens:\n",
    "            min_tokens = cantidad_tokens\n",
    "            llave_min_tokens = llave\n",
    "        if cantidad_tokens > max_tokens:\n",
    "            max_tokens = cantidad_tokens\n",
    "            llave_max_tokens = llave\n",
    "    return docs[llave_min_tokens], docs[llave_max_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_terminos(diccionario):\n",
    "    # Ordenar en orden ascendente para los términos menos frecuentes\n",
    "    dic_less_freq = sorted(diccionario.items(), key=lambda x: x[1])\n",
    "    # Ordenar en orden descendente para los términos más frecuentes\n",
    "    dic_more_freq = sorted(diccionario.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Seleccionar los 10 primeros términos\n",
    "    return dic_less_freq[:10], dic_more_freq[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTermsToFile(output_file: str, word_freq_total, word_freq_docs):\n",
    "    word_freq_total_ordered = dict(sorted(word_freq_total.items()))\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for term in word_freq_total_ordered:\n",
    "            file.write(f\"{term} {word_freq_total[term]} {word_freq_docs[term]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeStadisticsToFile(output_file: str, total_docs, total_tokens, total_terms, key_list_terms, shortestAndLargestDoc, one_time_terms):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"{total_docs}\\n\")\n",
    "        file.write(f\"{total_tokens} {total_terms}\\n\")\n",
    "        file.write(f\"{total_tokens * 100 / total_tokens} {total_terms * 100 / total_tokens}\\n\")\n",
    "        file.write(f\"{averageTermLength(key_list_terms)}\\n\")\n",
    "        file.write(f\"{shortestAndLargestDoc[0][0]} {shortestAndLargestDoc[0][1]} {shortestAndLargestDoc[1][0]} {shortestAndLargestDoc[1][1]}\\n\")\n",
    "        file.write(f\"{one_time_terms}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeFrequencyToFile(output_file: str, word_freq_docs, firstTenTerms):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for term, _ in firstTenTerms[0]:\n",
    "            file.write(f\"{term} {word_freq_docs[term]}\\n\")\n",
    "        for term, _ in firstTenTerms[1]:\n",
    "            file.write(f\"{term} {word_freq_docs[term]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir(directory: str, stopWordsFile = ''):\n",
    "    stop_words_list = loadFileStopWords(stopWordsFile) if stopWordsFile != '' else None\n",
    "    directory = os.path.abspath(directory)\n",
    "    total_tokens = 0\n",
    "    total_docs = len(os.listdir(directory))\n",
    "    word_freq_total = {}\n",
    "    word_freq_docs = {}\n",
    "    docs = {}\n",
    "    for file in os.listdir(directory):\n",
    "        complete_route = os.path.join(directory, file)\n",
    "        with open(complete_route, 'r', encoding='utf8') as f:\n",
    "            doc_count_terms = 0\n",
    "            content = f.read()\n",
    "            content_tokenized = tokenizer(content, stop_words_list)\n",
    "            total_tokens += len(content_tokenized)\n",
    "            unique_words_in_file = set(content_tokenized)\n",
    "            for word in unique_words_in_file:\n",
    "                word_freq_total[word] = word_freq_total.get(word, 0) + content_tokenized.count(word)\n",
    "                word_freq_docs[word] = word_freq_docs.get(word, 0) + 1\n",
    "                doc_count_terms += 1\n",
    "            docs[file] = (len(content_tokenized), doc_count_terms)\n",
    "    return word_freq_total, word_freq_docs, total_docs, total_tokens, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_freq_total, word_freq_docs = tokenizer(COLLECTION_DIR)\n",
    "word_freq_total, word_freq_docs, total_docs, total_tokens, docs = process_dir(COLLECTION_DIR, STOP_WORDS_FILE)\n",
    "writeTermsToFile('terminos.txt', word_freq_total, word_freq_docs)\n",
    "writeStadisticsToFile('estadisticas.txt', total_docs, total_tokens, len(word_freq_total), list(word_freq_total.keys()), shortestAndLargestDoc(docs), len([value for value in word_freq_total.values() if value == 1]))\n",
    "writeFrequencyToFile('frequencia.txt', word_freq_docs, obtener_terminos(word_freq_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
